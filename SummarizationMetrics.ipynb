{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#[ROUGE](https://aclanthology.org/W04-1013.pdf)"
      ],
      "metadata": {
        "id": "ZSikXjYVgyY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recall-Oriented Understudy for Gisting Evaluation**\n",
        "\n",
        "Set of metrics that measure the n-gram overlap between the generated summary and the reference summary.\n",
        "\n",
        "\n",
        "\n",
        "1.   **Rouge-N:** based on the count of overlapping n-gram units\n",
        "2.   **Rouge-L**:  based on the longest common subsequence between two summaries\n",
        "3.   **Rouge-W**: takes into account skip-bigram co-occurrence statistics\n",
        "4.   **Rouge-S**: based on skip-bigram co-occurrence statistics and measures the similarity between a candidate translation and a set of reference translations\n",
        "\n"
      ],
      "metadata": {
        "id": "zqGQsofQg7ih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sG9m9QpmWvQ",
        "outputId": "70472ecd-49ff-4914-b9e4-d819bbe95ba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_metric\n",
            "  Downloading rouge_metric-1.0.1-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rouge_metric\n",
            "Successfully installed rouge_metric-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_metric import PyRouge\n",
        "\n",
        "# Load summary results\n",
        "hypotheses = [\n",
        "    'how are you\\ni am fine',  # document 1: hypothesis\n",
        "    'it is fine today\\nwe won the football game',  # document 2: hypothesis\n",
        "]\n",
        "references = [[\n",
        "    'how do you do\\nfine thanks',  # document 1: reference 1\n",
        "    'how old are you\\ni am three',  # document 1: reference 2\n",
        "], [\n",
        "    'it is sunny today\\nlet us go for a walk',  # document 2: reference 1\n",
        "    'it is a terrible day\\nwe lost the game',  # document 2: reference 2\n",
        "]]\n",
        "\n",
        "# Evaluate document-wise ROUGE scores\n",
        "rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
        "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
        "scores = rouge.evaluate(hypotheses, references)\n",
        "print(scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBom4MYSg8vZ",
        "outputId": "7deebde9-b814-4849-932b-ea748c46bdb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.5182186234817814, 'p': 0.5555555555555556, 'f': 0.5362379555927943}, 'rouge-2': {'r': 0.19518716577540107, 'p': 0.2125, 'f': 0.20347597966879813}, 'rouge-4': {'r': 0.07142857142857142, 'p': 0.08333333333333333, 'f': 0.07692307692307691}, 'rouge-l': {'r': 0.5182186234817814, 'p': 0.5555555555555556, 'f': 0.5362379555927943}, 'rouge-w-1.2': {'r': 0.33608419409971513, 'p': 0.4734837712933738, 'f': 0.3931242798550236}, 'rouge-s4': {'r': 0.2549450549450549, 'p': 0.2916666666666667, 'f': 0.27207237393198186}, 'rouge-su4': {'r': 0.3149522799575822, 'p': 0.35526315789473684, 'f': 0.3338954468802698}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"rouge-1: {scores['rouge-1']}\")\n",
        "print(f\"rouge-2: {scores['rouge-2']}\")\n",
        "print(f\"rouge-4: {scores['rouge-4']}\")\n",
        "print(f\"rouge-l: {scores['rouge-l']}\")\n",
        "print(f\"rouge-w-1.2: {scores['rouge-w-1.2']}\")\n",
        "print(f\"rouge-s4: {scores['rouge-s4']}\")\n",
        "print(f\"rouge-su4: {scores['rouge-s4']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZtxzGJAn_zZ",
        "outputId": "da007738-0c33-4054-b003-2e926a8e759c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge-1: {'r': 0.5182186234817814, 'p': 0.5555555555555556, 'f': 0.5362379555927943}\n",
            "rouge-2: {'r': 0.19518716577540107, 'p': 0.2125, 'f': 0.20347597966879813}\n",
            "rouge-4: {'r': 0.07142857142857142, 'p': 0.08333333333333333, 'f': 0.07692307692307691}\n",
            "rouge-l: {'r': 0.5182186234817814, 'p': 0.5555555555555556, 'f': 0.5362379555927943}\n",
            "rouge-w-1.2: {'r': 0.33608419409971513, 'p': 0.4734837712933738, 'f': 0.3931242798550236}\n",
            "rouge-s4: {'r': 0.2549450549450549, 'p': 0.2916666666666667, 'f': 0.27207237393198186}\n",
            "rouge-su4: {'r': 0.2549450549450549, 'p': 0.2916666666666667, 'f': 0.27207237393198186}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "1. **r** represents **recall**, which is the proportion of words in the reference summary sentence that are also present in the candidate summary sentence.\n",
        "2. **p** represents **precision**, which is the proportion of words in the candidate summary sentence that are also present in the reference summary sentence\n",
        "3. **f** represents **F-measure**, which is a combined measure of recall and precision. F = 2 * (r * p) / (r + p)"
      ],
      "metadata": {
        "id": "aNj9IzdWyc5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_metric import PyRouge\n",
        "\n",
        "# Load summary results\n",
        "hypotheses = [\n",
        "    \"\"\"\n",
        "    Jorge Amado nascido in Itabuna, Brazil, 10 de agosto de 1912 – Salvador . Filiado ao Partido Comunista Brasileiro, por ele foi eleito Deputado Federal em 1946 . Teve outros três irmãos: Jofre, Joelson e James, único nacido em Ilhéus . Fled parte of a funda da \"Acadovação dos Rebeldes\", grupo de jovens que despencede importante .'\n",
        "    \"\"\"\n",
        "] #document 1\n",
        "references = [[\n",
        "    \"\"\"\n",
        "    Jorge Amado nascido in Itabuna, Brazil, 10 de agosto de 1912 – Salvador . Filiado a Partido Comunista Brasileiro, por ele foi eleito Deputado Federal em 1946 . Teve outros três irmãos: Jofre, Joelson e James\n",
        "    \"\"\"\n",
        "]]\n",
        "\n",
        "# Evaluate document-wise ROUGE scores\n",
        "rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=True,\n",
        "                rouge_w_weight=1.2, rouge_s=True, rouge_su=True, skip_gap=4)\n",
        "scores = rouge.evaluate(hypotheses, references)\n",
        "print(f\"rouge-1: {scores['rouge-1']}\")\n",
        "print(f\"rouge-2: {scores['rouge-2']}\")\n",
        "print(f\"rouge-4: {scores['rouge-4']}\")\n",
        "print(f\"rouge-l: {scores['rouge-l']}\")\n",
        "print(f\"rouge-w-1.2: {scores['rouge-w-1.2']}\")\n",
        "print(f\"rouge-s4: {scores['rouge-s4']}\")\n",
        "print(f\"rouge-su4: {scores['rouge-s4']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "outNBVj5MHD8",
        "outputId": "1f4c7534-88d8-483b-c5cc-a95bf6cc5d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge-1: {'r': 0.9722222222222222, 'p': 0.6140350877192983, 'f': 0.7526881720430108}\n",
            "rouge-2: {'r': 0.9142857142857143, 'p': 0.5714285714285714, 'f': 0.7032967032967032}\n",
            "rouge-4: {'r': 0.8484848484848485, 'p': 0.5185185185185185, 'f': 0.6436781609195402}\n",
            "rouge-l: {'r': 0.9444444444444444, 'p': 0.5964912280701754, 'f': 0.7311827956989247}\n",
            "rouge-w-1.2: {'r': 0.41147727286425834, 'p': 0.5321499161545168, 'f': 0.4640976834970099}\n",
            "rouge-s4: {'r': 0.9151515151515152, 'p': 0.5592592592592592, 'f': 0.6942528735632184}\n",
            "rouge-su4: {'r': 0.9151515151515152, 'p': 0.5592592592592592, 'f': 0.6942528735632184}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU"
      ],
      "metadata": {
        "id": "W_cn5L_Xg-k8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bilingual Evaluation Understudy**\n",
        "\n",
        "It measures the similarity between the generated summary and the reference summary in terms of n-grams."
      ],
      "metadata": {
        "id": "EV7NdsYLhCNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def evaluate_summary(reference_summaries, generated_summaries):\n",
        "    # Ensure that the reference summaries and generated summaries are lists of lists\n",
        "    # Each inner list corresponds to multiple reference summaries or a single generated summary\n",
        "    assert len(reference_summaries) == len(generated_summaries), \"Mismatched summary lengths\"\n",
        "\n",
        "    # Calculate the BLEU score\n",
        "    smoothie = SmoothingFunction().method4 # using smoothing method 4 as an example\n",
        "    bleu_score = corpus_bleu(reference_summaries, generated_summaries, smoothing_function=smoothie)\n",
        "\n",
        "    return bleu_score\n",
        "\n",
        "# Example Usage\n",
        "reference_summaries = [[\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"A quick brown fox leaps over a lazy dog\"\n",
        "]]\n",
        "\n",
        "generated_summaries = [\n",
        "    \"A speedy brown fox jumps over the lazy dog\"\n",
        "]\n",
        "\n",
        "bleu_score = evaluate_summary(reference_summaries, generated_summaries)\n",
        "print(f'BLEU Score: {bleu_score:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RafBioKdhBc7",
        "outputId": "cfb9c14e-9600-4e6d-d704-d41336380b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-5 (attachment_entry):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 237, in listen\n",
            "    sock, _ = endpoints_listener.accept()\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 293, in accept\n",
            "    fd, addr = self._accept()\n",
            "TimeoutError: timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/google/colab/_debugpy.py\", line 52, in attachment_entry\n",
            "    debugpy.listen(_dap_port)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/public_api.py\", line 31, in wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 143, in debug\n",
            "    log.reraise_exception(\"{0}() failed:\", func.__name__, level=\"info\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 141, in debug\n",
            "    return func(address, settrace_kwargs, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/debugpy/server/api.py\", line 251, in listen\n",
            "    raise RuntimeError(\"timed out waiting for adapter to connect\")\n",
            "RuntimeError: timed out waiting for adapter to connect\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 0.81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERTScore"
      ],
      "metadata": {
        "id": "EVkJY0eghFEQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metric for evaluating text generation tasks, such as summarization. It computes token-level cosine similarity between embeddings of the generated text and a reference text using BERT embeddings"
      ],
      "metadata": {
        "id": "1H6AL4GWhGrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert-score"
      ],
      "metadata": {
        "id": "Mrk7rzdpTir8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f257df14-288c-4ea2-c4b0-8e8f805d6666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.33.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (4.66.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert-score) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert-score) (16.0.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.17.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert-score) (0.3.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score) (2023.7.22)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=3.0.0->bert-score) (2023.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.0.1->bert-score) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score\n",
        "\n",
        "def evaluate_summary(generated_summary, reference_summary):\n",
        "    # Compute BERTScore\n",
        "    P, R, F1 = score([generated_summary], [reference_summary], lang='en', model_type='bert-base-uncased', num_layers=9, rescale_with_baseline=True)\n",
        "\n",
        "    # Convert tensors to numpy arrays for easier handling\n",
        "    P = P.numpy()[0]\n",
        "    R = R.numpy()[0]\n",
        "    F1 = F1.numpy()[0]\n",
        "\n",
        "    return P, R, F1\n",
        "\n",
        "# Example usage:\n",
        "generated_summary = \"The AI model performed exceedingly well on the given task.\"\n",
        "reference_summary = \"The given task was performed exceedingly well by the AI model.\"\n",
        "\n",
        "precision, recall, f1_score = evaluate_summary(generated_summary, reference_summary)\n",
        "\n",
        "print(f'Precision: {precision:.3f}, Recall: {recall:.3f}, F1 Score: {f1_score:.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKD-aYtbhHEm",
        "outputId": "a9ad5fd7-7811-459d-cfb9-f38c64406a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.796, Recall: 0.778, F1 Score: 0.788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# METEOR"
      ],
      "metadata": {
        "id": "N0KGYW1whIyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Metric for Evaluation of Translation with Explicit ORdering**\n",
        "\n",
        "Designed to evaluate machine translation systems, but can also be applied to summarization tasks. It's one of the metrics that tries to overcome the limitations of BLEU by considering synonyms, stemming, and word order when comparing the generated text to a reference text.\n",
        "\n",
        "In METEOR, a score is calculated based on the harmonic mean of precision and recall between the generated text and the reference(s), with several additional considerations:\n",
        "\n",
        "1.  Exact match: Words in the generated text and reference that match exactly.\n",
        "2.  Stem match: Words that match after stemming.\n",
        "3.  Synonym match: Words that are synonyms but not exact matches.\n",
        "4.  Word order: The order of matching words."
      ],
      "metadata": {
        "id": "aH96eBv_hJrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate import meteor_score\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "# Assume `generated_summary` and `reference_summary` are your variables\n",
        "generated_summary = \"This is a summary generated by your model.\"\n",
        "reference_summary = \"This is the reference summary.\"\n",
        "\n",
        "# Tokenize the summaries\n",
        "tokenized_generated_summary = word_tokenize(generated_summary)\n",
        "tokenized_reference_summary = word_tokenize(reference_summary)\n",
        "\n",
        "# Now compute the METEOR score\n",
        "score = meteor_score.single_meteor_score(tokenized_reference_summary, tokenized_generated_summary)\n",
        "\n",
        "print(f'METEOR score: {score:.2f}')\n"
      ],
      "metadata": {
        "id": "UNcp5wbshKCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06f0378b-2606-41d5-c894-7332dade61a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR score: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MOVERScore"
      ],
      "metadata": {
        "id": "wJhTwTpghLT-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matching Overlap of Variable-length n-grams with Embedding Re-ranking**\n",
        "\n",
        "The metric compares a generated summary to a reference summary, analyzing both the content overlap and the semantic similarity. Here's how it generally works:"
      ],
      "metadata": {
        "id": "rcOxWHY9hMQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moverscore\n",
        "!pip install pyemd\n",
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DZHtIK6PFrG",
        "outputId": "ebdaeb44-4aeb-4983-c651-80028f41fd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moverscore in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.10/dist-packages (from moverscore) (3.7.4.3)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from moverscore) (2.8.2)\n",
            "Requirement already satisfied: pyemd in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from pyemd) (1.23.5)\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.10/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.23.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.28.57)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2023.6.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=0.4.1->pytorch_pretrained_bert) (16.0.6)\n",
            "Requirement already satisfied: botocore<1.32.0,>=1.31.57 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.31.57)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from boto3->pytorch_pretrained_bert) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.57->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.32.0,>=1.31.57->boto3->pytorch_pretrained_bert) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moverscore import get_idf_dict, word_mover_score\n",
        "import numpy as np\n",
        "\n",
        "# Define your reference and candidate summaries\n",
        "references = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The quick brown fox jumped over the lazy dog.\",\n",
        "    \"A journey of a thousand miles begins with a single step.\",\n",
        "    \"To be or not to be, that is the question.\",\n",
        "    \"All that glitters is not gold.\"\n",
        "]\n",
        "\n",
        "candidates = [\n",
        "    \"A cat sat on a mat.\",\n",
        "    \"A fast, dark-colored fox leapt over a slow, lazy dog.\",\n",
        "    \"A long trip begins with one step.\",\n",
        "    \"To exist or not to exist, that's the inquiry.\",\n",
        "    \"Not everything that shines is made of gold.\"\n",
        "]\n",
        "\n",
        "# Precompute IDF dictionaries\n",
        "idf_dict_ref = get_idf_dict(references)  # or provide your own IDF dictionary\n",
        "idf_dict_hyp = get_idf_dict(candidates)  # or provide your own IDF dictionary\n",
        "\n",
        "# Compute MOVERScore\n",
        "scores = word_mover_score(references, candidates, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords=True)\n",
        "\n",
        "# Convert scores to a numpy array and print the mean MOVERScore\n",
        "scores = np.array(scores)\n",
        "print(f'Mean MOVERScore: {scores.mean():.4f}')\n"
      ],
      "metadata": {
        "id": "LHRXpc9zhMb-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a0e4eb-6ec4-488a-b799-2ee7026df545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean MOVERScore: 0.5540\n"
          ]
        }
      ]
    }
  ]
}